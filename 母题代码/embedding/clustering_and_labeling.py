"""
Schema èšç±»ä¸è¯­ä¹‰æ ‡ç­¾æå–ç³»ç»Ÿ

ç›®çš„ï¼š
1. å°†å·²å‘é‡åŒ–çš„é¢˜ç›®Schemaèšç±»ï¼Œå‘ç°é¢˜ç›®ä¹‹é—´çš„ç»“æ„ç›¸ä¼¼æ€§
2. ä¸ºæ¯ä¸ªèšç±»è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰æ ‡ç­¾ï¼ˆç®—æ³•ç±»å‹ã€è§£é¢˜ç­–ç•¥ç­‰ï¼‰
3. æ„å»ºé¢˜ç›®å…³ç³»å›¾è°±ï¼Œæ”¯æŒçŸ¥è¯†ç®¡ç†ä¸æ¨è
4. è¯„ä¼°èšç±»è´¨é‡ï¼Œä¼˜åŒ–èšç±»å‚æ•°

è¾“å‡ºï¼š
- èšç±»ç»“æœï¼ˆåŒ…å«ç°‡IDã€ä»£è¡¨é¢˜ã€æ ‡ç­¾ç­‰ï¼‰
- æ ‡ç­¾åº“ï¼ˆè‡ªåŠ¨ç”Ÿæˆçš„ç®—æ³•æ ‡ç­¾åŠæƒé‡ï¼‰
- èšç±»è´¨é‡è¯„ä¼°æŠ¥å‘Š
- å¯è§†åŒ–æ•°æ®ï¼ˆ2D/3Då±•ç¤ºï¼‰
"""

import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional
from dataclasses import dataclass, asdict
from collections import Counter, defaultdict
import warnings
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ åº“
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from sklearn.metrics import (
    silhouette_score, davies_bouldin_score, calinski_harabasz_score,
    silhouette_samples
)
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage

# æ–‡æœ¬åˆ†æåº“
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter

# å¯é€‰ï¼šä¸­æ–‡åˆ†è¯
try:
    import jieba
except ImportError:
    jieba = None

import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams

# è®¾ç½®ä¸­æ–‡æ˜¾ç¤º
rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'WenQuanYi Zen Hei']
rcParams['axes.unicode_minus'] = False

import config


@dataclass
class ClusterLabel:
    """èšç±»æ ‡ç­¾"""
    cluster_id: int
    primary_label: str          # ä¸»è¦æ ‡ç­¾ï¼ˆå¦‚"åŠ¨æ€è§„åˆ’"ï¼‰
    secondary_labels: List[str] # æ¬¡è¦æ ‡ç­¾ï¼ˆå¦‚["ä¸€ç»´", "æœ€ä¼˜åŒ–"]ï¼‰
    label_confidence: float     # æ ‡ç­¾ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
    size: int                   # èšç±»å¤§å°
    examples: List[Dict]        # ä»£è¡¨é¢˜ç›®


@dataclass
class ClusteringResult:
    """èšç±»ç»“æœ"""
    n_clusters: int
    labels: np.ndarray          # æ¯ä¸ªæ ·æœ¬çš„èšç±»æ ‡ç­¾
    centers: np.ndarray         # èšç±»ä¸­å¿ƒ
    silhouette_score: float     # è½®å»“ç³»æ•°
    davies_bouldin_score: float # DBæŒ‡æ•°
    calinski_harabasz_score: float # CHæŒ‡æ•°
    cluster_labels: List[ClusterLabel] # æ¯ä¸ªèšç±»çš„æ ‡ç­¾


class SchemaClusterer:
    """Schemaèšç±»ä¸æ ‡ç­¾æå–ç³»ç»Ÿ"""
    
    def __init__(self, embeddings: np.ndarray, metadata: np.ndarray, 
                 schemas: List[Dict] = None):
        """
        åˆå§‹åŒ–èšç±»å™¨
        
        Args:
            embeddings: NÃ—D å‘é‡çŸ©é˜µï¼ˆNä¸ªSchemaï¼ŒDç»´å‘é‡ï¼‰
            metadata: Nç»´å…ƒæ•°æ®æ•°ç»„ï¼ˆåŒ…å«title, slugç­‰ï¼‰
            schemas: åŸå§‹Schemaåˆ—è¡¨ï¼ˆç”¨äºæ ‡ç­¾æå–ï¼‰
        """
        self.embeddings = embeddings
        self.metadata = metadata
        self.schemas = schemas or []
        self.n_samples = len(embeddings)
        
        print(f"âœ“ å·²åŠ è½½ {self.n_samples} ä¸ªSchemaçš„å‘é‡è¡¨ç¤º")
        print(f"  å‘é‡ç»´åº¦: {embeddings.shape[1]}")
    
    def find_optimal_k(self, k_range: range = range(5, 51), 
                       method: str = 'silhouette') -> int:
        """
        ä½¿ç”¨å¤šç§æŒ‡æ ‡å¯»æ‰¾æœ€ä¼˜èšç±»æ•°
        
        Args:
            k_range: è¦æµ‹è¯•çš„èšç±»æ•°èŒƒå›´
            method: è¯„ä¼°æ–¹æ³• ('silhouette', 'davies_bouldin', 'elbow')
        
        Returns:
            æœ€ä¼˜çš„èšç±»æ•°
        """
        print(f"\nğŸ” å¯»æ‰¾æœ€ä¼˜èšç±»æ•° (KèŒƒå›´: {k_range.start}-{k_range.stop})...")
        
        scores_silhouette = []
        scores_davies_bouldin = []
        scores_calinski = []
        
        for k in k_range:
            print(f"  æµ‹è¯• K={k}...", end='')
            
            # K-meansèšç±»
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(self.embeddings)
            
            # è®¡ç®—æŒ‡æ ‡
            sil_score = silhouette_score(self.embeddings, labels)
            db_score = davies_bouldin_score(self.embeddings, labels)
            ch_score = calinski_harabasz_score(self.embeddings, labels)
            
            scores_silhouette.append(sil_score)
            scores_davies_bouldin.append(db_score)
            scores_calinski.append(ch_score)
            
            print(f" è½®å»“ç³»æ•°={sil_score:.3f}, DBæŒ‡æ•°={db_score:.3f}")
        
        # æ ¹æ®æ–¹æ³•é€‰æ‹©æœ€ä¼˜K
        if method == 'silhouette':
            optimal_k = list(k_range)[np.argmax(scores_silhouette)]
        elif method == 'davies_bouldin':
            optimal_k = list(k_range)[np.argmin(scores_davies_bouldin)]
        else:  # calinski
            optimal_k = list(k_range)[np.argmax(scores_calinski)]
        
        print(f"\nâœ“ æ¨èèšç±»æ•°: K={optimal_k}")
        
        # ç»˜åˆ¶æŒ‡æ ‡æ›²çº¿
        self._plot_k_selection(k_range, scores_silhouette, 
                               scores_davies_bouldin, scores_calinski)
        
        return optimal_k
    
    def _plot_k_selection(self, k_range, sil_scores, db_scores, ch_scores):
        """ç»˜åˆ¶Kå€¼é€‰æ‹©æ›²çº¿"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        
        k_list = list(k_range)
        
        # è½®å»“ç³»æ•°
        axes[0].plot(k_list, sil_scores, 'o-', linewidth=2, markersize=6)
        axes[0].set_xlabel('èšç±»æ•° K')
        axes[0].set_ylabel('è½®å»“ç³»æ•° (è¶Šå¤§è¶Šå¥½)')
        axes[0].set_title('è½®å»“ç³»æ•°')
        axes[0].grid(True, alpha=0.3)
        
        # DBæŒ‡æ•°
        axes[1].plot(k_list, db_scores, 's-', linewidth=2, markersize=6, color='orange')
        axes[1].set_xlabel('èšç±»æ•° K')
        axes[1].set_ylabel('DBæŒ‡æ•° (è¶Šå°è¶Šå¥½)')
        axes[1].set_title('Davies-BouldinæŒ‡æ•°')
        axes[1].grid(True, alpha=0.3)
        
        # CHæŒ‡æ•°
        axes[2].plot(k_list, ch_scores, '^-', linewidth=2, markersize=6, color='green')
        axes[2].set_xlabel('èšç±»æ•° K')
        axes[2].set_ylabel('CHæŒ‡æ•° (è¶Šå¤§è¶Šå¥½)')
        axes[2].set_title('Calinski-HarabaszæŒ‡æ•°')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        output_path = config.OUTPUT_DIR / 'k_selection_curves.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ Kå€¼é€‰æ‹©æ›²çº¿å·²ä¿å­˜: {output_path}")
        plt.close()
    
    def cluster_kmeans(self, n_clusters: int = None) -> ClusteringResult:
        """
        æ‰§è¡ŒK-meansèšç±»
        
        Args:
            n_clusters: èšç±»æ•°ï¼ˆå¦‚æœä¸ºNoneï¼Œè‡ªåŠ¨å¯»æ‰¾æœ€ä¼˜å€¼ï¼‰
        
        Returns:
            ClusteringResultå¯¹è±¡
        """
        if n_clusters is None:
            n_clusters = self.find_optimal_k()
        
        print(f"\nğŸ”„ æ‰§è¡ŒK-meansèšç±» (K={n_clusters})...")
        
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, max_iter=300)
        labels = kmeans.fit_predict(self.embeddings)
        centers = kmeans.cluster_centers_
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        sil_score = silhouette_score(self.embeddings, labels)
        db_score = davies_bouldin_score(self.embeddings, labels)
        ch_score = calinski_harabasz_score(self.embeddings, labels)
        
        print(f"âœ“ èšç±»å®Œæˆ:")
        print(f"  è½®å»“ç³»æ•°: {sil_score:.4f} (èŒƒå›´: [-1, 1], è¶Šæ¥è¿‘1è¶Šå¥½)")
        print(f"  DBæŒ‡æ•°: {db_score:.4f} (è¶Šå°è¶Šå¥½)")
        print(f"  CHæŒ‡æ•°: {ch_score:.4f} (è¶Šå¤§è¶Šå¥½)")
        
        # ä¸ºæ¯ä¸ªèšç±»ç”Ÿæˆæ ‡ç­¾
        cluster_labels = self._generate_cluster_labels(labels, n_clusters)
        
        return ClusteringResult(
            n_clusters=n_clusters,
            labels=labels,
            centers=centers,
            silhouette_score=sil_score,
            davies_bouldin_score=db_score,
            calinski_harabasz_score=ch_score,
            cluster_labels=cluster_labels
        )
    
    def _generate_cluster_labels(self, labels: np.ndarray, 
                                 n_clusters: int) -> List[ClusterLabel]:
        """
        ä¸ºæ¯ä¸ªèšç±»è‡ªåŠ¨ç”Ÿæˆè¯­ä¹‰æ ‡ç­¾
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        1. æå–èšç±»å†…æ‰€æœ‰Schemaçš„ç®—æ³•ç‰¹å¾ï¼ˆä¸å˜é‡ã€çº¦æŸç­‰ï¼‰
        2. ç»Ÿè®¡ç‰¹å¾é¢‘ç‡ï¼Œå¾—åˆ°è¯¥èšç±»çš„"ç‰¹å¾æŒ‡çº¹"
        3. åŸºäºç‰¹å¾æŒ‡çº¹ç”Ÿæˆè¯­ä¹‰æ ‡ç­¾
        """
        print(f"\nğŸ“ ä¸º {n_clusters} ä¸ªèšç±»ç”Ÿæˆæ ‡ç­¾...")
        
        cluster_labels = []
        
        for cluster_id in range(n_clusters):
            # è·å–è¯¥èšç±»çš„æ‰€æœ‰æ ·æœ¬ç´¢å¼•
            cluster_indices = np.where(labels == cluster_id)[0]
            cluster_size = len(cluster_indices)
            
            # æå–è¯¥èšç±»çš„ç‰¹å¾
            cluster_features = self._extract_cluster_features(cluster_indices)
            
            # ç”Ÿæˆæ ‡ç­¾
            primary_label, secondary_labels, confidence = \
                self._generate_labels_from_features(cluster_features)
            
            # é€‰æ‹©ä»£è¡¨é¢˜ç›®ï¼ˆèšç±»ä¸­å¿ƒæœ€è¿‘çš„å‡ ä¸ªï¼‰
            center = self.embeddings[cluster_indices].mean(axis=0)
            distances = np.linalg.norm(
                self.embeddings[cluster_indices] - center, axis=1
            )
            example_indices = cluster_indices[np.argsort(distances)[:3]]
            
            examples = [
                {
                    'title': self.metadata[idx]['title'],
                    'slug': self.metadata[idx]['slug'],
                    'distance_to_center': float(distances[np.where(cluster_indices == idx)[0][0]])
                }
                for idx in example_indices
            ]
            
            label_obj = ClusterLabel(
                cluster_id=cluster_id,
                primary_label=primary_label,
                secondary_labels=secondary_labels,
                label_confidence=confidence,
                size=cluster_size,
                examples=examples
            )
            cluster_labels.append(label_obj)
            
            print(f"  ç°‡{cluster_id}: {primary_label} ({cluster_size}é¢˜) "
                  f"- ç½®ä¿¡åº¦{confidence:.1%}")
        
        return cluster_labels
    
    def _extract_cluster_features(self, indices: np.ndarray) -> Dict[str, Counter]:
        """
        ä»èšç±»å†…çš„Schemaæå–ç‰¹å¾
        
        è¿”å›ç‰¹å¾è®¡æ•°å™¨ï¼Œä¾¿äºç»Ÿè®¡æœ€å¸¸è§çš„ç‰¹å¾
        """
        features = {
            'invariants': Counter(),
            'constraints': Counter(),
            'input_types': Counter(),
            'objectives': Counter()
        }
        
        for idx in indices:
            if idx < len(self.schemas):
                schema_item = self.schemas[idx]
                
                # å¤„ç†schemaç»“æ„ï¼šå¯èƒ½æ˜¯ {"schema": {...}} æˆ–ç›´æ¥æ˜¯ {...}
                if isinstance(schema_item, dict):
                    if 'schema' in schema_item:
                        schema = schema_item.get('schema', {})
                    else:
                        schema = schema_item
                else:
                    continue
                
                if not isinstance(schema, dict):
                    continue
                
                # æå–ç®—æ³•ä¸å˜é‡
                invariants = schema.get('Algorithmic Invariant', [])
                if isinstance(invariants, list):
                    for inv in invariants:
                        key = self._simplify_feature(inv)
                        if key:
                            features['invariants'][key] += 1
                
                # æå–æ ¸å¿ƒçº¦æŸ
                constraints = schema.get('Core Constraint', [])
                if isinstance(constraints, list):
                    for con in constraints[:2]:
                        key = self._simplify_feature(con)
                        if key:
                            features['constraints'][key] += 1
                
                # æå–è¾“å…¥ç»“æ„
                input_struct = schema.get('Input Structure', [])
                if isinstance(input_struct, list) and len(input_struct) > 0:
                    inp = input_struct[0]
                    key = self._simplify_feature(inp)
                    if key:
                        features['input_types'][key] += 1
                
                # æå–ç›®æ ‡å‡½æ•°
                objective = schema.get('Objective Function', '')
                if objective:
                    key = self._simplify_feature(objective)
                    if key:
                        features['objectives'][key] += 1
        
        return features
    
    def _simplify_feature(self, text: str) -> str:
        """
        ç®€åŒ–ç‰¹å¾æ–‡æœ¬ï¼Œæå–å…³é”®è¯
        ä¾‹å¦‚ï¼š
        - "åŒæŒ‡é’ˆç§»åŠ¨ï¼ŒåŒºé—´åˆæ³•æ€§å•è°ƒ" â†’ "åŒæŒ‡é’ˆ"
        - "é•¿åº¦ä¸º n çš„æ•°ç»„ A[1..n]" â†’ "æ•°ç»„"
        """
        if not isinstance(text, str):
            return ""
        
        text = text.lower()
        
        # å®šä¹‰å…³é”®è¯æ˜ å°„
        keywords_map = {
            'dp': ['åŠ¨æ€è§„åˆ’', 'dp', 'åŠ¨è§„'],
            'åŒæŒ‡é’ˆ': ['åŒæŒ‡é’ˆ', 'two pointer', 'æŒ‡é’ˆ'],
            'æ»‘åŠ¨çª—å£': ['æ»‘åŠ¨çª—å£', 'sliding window', 'çª—å£'],
            'åˆ†æ²»': ['åˆ†æ²»', 'åˆ†è€Œæ²»ä¹‹', 'divide and conquer'],
            'è´ªå¿ƒ': ['è´ªå¿ƒ', 'greedy'],
            'å›¾è®º': ['å›¾', 'graph', 'bfs', 'dfs', 'æœ€çŸ­è·¯'],
            'æ ‘': ['æ ‘', 'tree', 'äºŒå‰æ ‘', 'éå†'],
            'å‰ç¼€å’Œ': ['å‰ç¼€å’Œ', 'prefix sum'],
            'äºŒåˆ†æŸ¥æ‰¾': ['äºŒåˆ†', 'binary search', 'æŸ¥æ‰¾'],
            'æ’åº': ['æ’åº', 'sort'],
            'å­—ç¬¦ä¸²': ['å­—ç¬¦ä¸²', 'string', 'substring'],
            'æ•°ç»„': ['æ•°ç»„', 'array', 'sequence'],
            'è®¡æ•°': ['è®¡æ•°', 'count'],
            'æœ€å¤§å€¼': ['æœ€å¤§', 'maximum', 'max'],
            'æœ€å°å€¼': ['æœ€å°', 'minimum', 'min'],
            'æ±‚å’Œ': ['å’Œ', 'sum'],
        }
        
        for category, keywords in keywords_map.items():
            for keyword in keywords:
                if keyword in text:
                    return category
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œè¿”å›å‰20ä¸ªå­—ç¬¦
        return text[:20] if text else ""
    
    def _generate_labels_from_features(self, features: Dict[str, Counter]) \
            -> Tuple[str, List[str], float]:
        """
        åŸºäºç‰¹å¾è®¡æ•°ç”Ÿæˆæ ‡ç­¾
        
        Returns:
            (ä¸»æ ‡ç­¾, æ¬¡æ ‡ç­¾åˆ—è¡¨, ç½®ä¿¡åº¦)
        """
        # ç®—æ³•ç±»å‹ä¼˜å…ˆçº§æ˜ å°„
        algorithm_keywords = {
            'åŒæŒ‡é’ˆ': ['åŒæŒ‡é’ˆ', 'æ»‘åŠ¨çª—å£'],
            'åŠ¨æ€è§„åˆ’': ['dp', 'åŠ¨æ€è§„åˆ’'],
            'åˆ†æ²»': ['åˆ†æ²»'],
            'è´ªå¿ƒ': ['è´ªå¿ƒ'],
            'å›¾è®º': ['å›¾è®º', 'æœ€çŸ­è·¯', 'bfs', 'dfs'],
            'æ ‘å½¢DP': ['æ ‘', 'dp'],
            'å‰ç¼€å’Œ': ['å‰ç¼€å’Œ'],
            'äºŒåˆ†': ['äºŒåˆ†æŸ¥æ‰¾', 'äºŒåˆ†'],
        }
        
        # ç»Ÿè®¡ç®—æ³•å‡ºç°æ¬¡æ•°
        invariants = features['invariants']
        constraints = features['constraints']
        
        # æå–ä¸»è¦ç®—æ³•æ ‡ç­¾
        primary_label = 'æœªåˆ†ç±»'
        max_count = 0
        
        for algo, keywords in algorithm_keywords.items():
            count = sum(invariants.get(kw, 0) for kw in keywords)
            if count > max_count:
                max_count = count
                primary_label = algo
        
        # æå–æ¬¡è¦æ ‡ç­¾ï¼ˆè¾“å…¥/çº¦æŸç‰¹å¾ï¼‰
        secondary_labels = []
        
        input_top = features['input_types'].most_common(1)
        if input_top:
            secondary_labels.append(f"è¾“å…¥:{input_top[0][0]}")
        
        constraints_top = features['constraints'].most_common(2)
        for con, count in constraints_top:
            if count >= 2:
                secondary_labels.append(f"çº¦æŸ:{con}")
        
        objectives_top = features['objectives'].most_common(1)
        if objectives_top:
            secondary_labels.append(f"ç›®æ ‡:{objectives_top[0][0]}")
        
        # è®¡ç®—ç½®ä¿¡åº¦
        cluster_size = max_count
        confidence = min(0.95, 0.5 + cluster_size / 20)  # ç¤ºä¾‹ç½®ä¿¡åº¦è®¡ç®—
        
        return primary_label, secondary_labels[:2], confidence
    
    def visualize_clusters_2d(self, labels: np.ndarray, 
                             cluster_labels: List[ClusterLabel] = None,
                             method: str = 'tsne'):
        """
        2Då¯è§†åŒ–èšç±»ç»“æœ
        
        Args:
            labels: èšç±»æ ‡ç­¾
            cluster_labels: æ ‡ç­¾ä¿¡æ¯ï¼ˆç”¨äºæ³¨é‡Šï¼‰
            method: é™ç»´æ–¹æ³• ('tsne' æˆ– 'pca')
        """
        print(f"\nğŸ“Š ä½¿ç”¨{method.upper()}é™ç»´åˆ°2Dè¿›è¡Œå¯è§†åŒ–...")
        
        if method == 'tsne':
            reducer = TSNE(n_components=2, random_state=42, perplexity=min(30, self.n_samples-1))
            coords = reducer.fit_transform(self.embeddings)
            method_name = 't-SNE'
        else:
            reducer = PCA(n_components=2, random_state=42)
            coords = reducer.fit_transform(self.embeddings)
            method_name = 'PCA'
        
        # ç»˜åˆ¶
        fig, ax = plt.subplots(figsize=(14, 10))
        
        unique_labels = np.unique(labels)
        colors = plt.cm.tab20(np.linspace(0, 1, len(unique_labels)))
        
        for cluster_id, color in zip(unique_labels, colors):
            mask = labels == cluster_id
            ax.scatter(coords[mask, 0], coords[mask, 1], 
                      c=[color], label=f'ç°‡{cluster_id}', 
                      s=100, alpha=0.6, edgecolors='black', linewidth=0.5)
        
        # æ·»åŠ æ ‡ç­¾ä¿¡æ¯
        if cluster_labels:
            for label_info in cluster_labels:
                cluster_id = label_info.cluster_id
                mask = labels == cluster_id
                cluster_coords = coords[mask]
                center = cluster_coords.mean(axis=0)
                
                ax.annotate(
                    label_info.primary_label,
                    xy=center, xytext=(5, 5),
                    textcoords='offset points',
                    fontsize=10, fontweight='bold',
                    bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.5)
                )
        
        ax.set_xlabel(f'{method_name} ç¬¬ä¸€ä¸»æˆåˆ†', fontsize=12)
        ax.set_ylabel(f'{method_name} ç¬¬äºŒä¸»æˆåˆ†', fontsize=12)
        ax.set_title(f'Schema èšç±»å¯è§†åŒ– ({method_name})', fontsize=14, fontweight='bold')
        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        output_path = config.OUTPUT_DIR / f'clustering_visualization_{method}.png'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ èšç±»å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
        plt.close()
    
    def generate_report(self, result: ClusteringResult) -> str:
        """
        ç”Ÿæˆèšç±»åˆ†ææŠ¥å‘Š
        """
        report = []
        report.append("=" * 80)
        report.append("Schema èšç±»åˆ†ææŠ¥å‘Š".center(80))
        report.append("=" * 80)
        report.append("")
        
        # åŸºæœ¬ç»Ÿè®¡
        report.append(f"æ€»æ ·æœ¬æ•°: {self.n_samples}")
        report.append(f"èšç±»æ•°: {result.n_clusters}")
        report.append(f"å¹³å‡æ¯ä¸ªèšç±»çš„å¤§å°: {self.n_samples // result.n_clusters}")
        report.append("")
        
        # èšç±»è´¨é‡æŒ‡æ ‡
        report.append("ã€èšç±»è´¨é‡è¯„ä¼°ã€‘")
        report.append(f"  è½®å»“ç³»æ•° (Silhouette): {result.silhouette_score:.4f}")
        report.append(f"    è¯´æ˜: èŒƒå›´[-1, 1]ï¼Œè¶Šæ¥è¿‘1è¶Šå¥½ã€‚å½“å‰å€¼è¡¨ç¤ºèšç±»æ•ˆæœ{'ä¼˜ç§€' if result.silhouette_score > 0.5 else 'ä¸€èˆ¬' if result.silhouette_score > 0.3 else 'éœ€æ”¹è¿›'}")
        report.append(f"  Davies-BouldinæŒ‡æ•° (DB Index): {result.davies_bouldin_score:.4f}")
        report.append(f"    è¯´æ˜: è¶Šå°è¶Šå¥½ï¼Œè¡¨ç¤ºèšç±»ä¹‹é—´çš„åˆ†ç¦»åº¦è¾ƒå¥½" if result.davies_bouldin_score < 1.5 else "    è¯´æ˜: èšç±»åˆ†ç¦»åº¦ä¸€èˆ¬ï¼Œå»ºè®®ä¼˜åŒ–å‚æ•°")
        report.append(f"  Calinski-HarabaszæŒ‡æ•° (CH Index): {result.calinski_harabasz_score:.2f}")
        report.append(f"    è¯´æ˜: è¶Šå¤§è¶Šå¥½ï¼Œè¡¨ç¤ºèšç±»å†…ç´§å‡‘æ€§å¥½ï¼Œèšç±»é—´åˆ†ç¦»åº¦å¤§")
        report.append("")
        
        # èšç±»æ ‡ç­¾ä¿¡æ¯
        report.append("ã€èšç±»æ ‡ç­¾ä¿¡æ¯ã€‘")
        report.append(f"{'ç°‡ID':<5} {'æ ‡ç­¾':<15} {'ç½®ä¿¡åº¦':<8} {'å¤§å°':<6} {'ä»£è¡¨é¢˜ç›®':<30}")
        report.append("-" * 70)
        
        for label_info in sorted(result.cluster_labels, key=lambda x: x.size, reverse=True):
            examples_str = " / ".join([ex['title'][:10] for ex in label_info.examples[:2]])
            report.append(
                f"{label_info.cluster_id:<5} "
                f"{label_info.primary_label:<15} "
                f"{label_info.label_confidence:.1%}{'':<2} "
                f"{label_info.size:<6} "
                f"{examples_str:<30}"
            )
        
        report.append("")
        
        # èšç±»è¯¦ç»†ä¿¡æ¯
        report.append("ã€èšç±»è¯¦ç»†ä¿¡æ¯ã€‘")
        for label_info in sorted(result.cluster_labels, key=lambda x: x.size, reverse=True):
            report.append(f"\nã€ç°‡{label_info.cluster_id}ã€‘: {label_info.primary_label}")
            report.append(f"  å¤§å°: {label_info.size}é“é¢˜")
            report.append(f"  ç½®ä¿¡åº¦: {label_info.label_confidence:.1%}")
            if label_info.secondary_labels:
                report.append(f"  ç‰¹å¾æ ‡ç­¾: {', '.join(label_info.secondary_labels)}")
            report.append(f"  ä»£è¡¨é¢˜ç›®:")
            for example in label_info.examples:
                report.append(f"    - {example['title']} (è·ç°‡å¿ƒè·ç¦»: {example['distance_to_center']:.3f})")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)
    
    def save_results(self, result: ClusteringResult):
        """ä¿å­˜èšç±»ç»“æœ"""
        
        # ä¿å­˜èšç±»æ ‡ç­¾ç»“æœï¼ˆJSONï¼‰
        labels_data = {
            'n_clusters': result.n_clusters,
            'quality_metrics': {
                'silhouette_score': float(result.silhouette_score),
                'davies_bouldin_score': float(result.davies_bouldin_score),
                'calinski_harabasz_score': float(result.calinski_harabasz_score)
            },
            'clusters': [
                {
                    'cluster_id': label.cluster_id,
                    'primary_label': label.primary_label,
                    'secondary_labels': label.secondary_labels,
                    'confidence': label.label_confidence,
                    'size': label.size,
                    'examples': label.examples
                }
                for label in result.cluster_labels
            ]
        }
        
        output_path = config.OUTPUT_DIR / 'clustering_labels.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(labels_data, f, ensure_ascii=False, indent=2)
        print(f"âœ“ èšç±»æ ‡ç­¾å·²ä¿å­˜: {output_path}")
        
        # ä¿å­˜èšç±»åˆ†é…ç»“æœ
        assignments = {
            'sample_count': len(result.labels),
            'assignments': {
                int(i): int(result.labels[i])
                for i in range(len(result.labels))
            }
        }
        output_path = config.OUTPUT_DIR / 'clustering_assignments.json'
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(assignments, f, ensure_ascii=False, indent=2)
        print(f"âœ“ èšç±»åˆ†é…å·²ä¿å­˜: {output_path}")
        
        # ä¿å­˜æŠ¥å‘Š
        report = self.generate_report(result)
        output_path = config.OUTPUT_DIR / 'clustering_report.txt'
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"âœ“ èšç±»æŠ¥å‘Šå·²ä¿å­˜: {output_path}")
        
        print("\næŠ¥å‘Šå†…å®¹:")
        print(report)


def load_data():
    """åŠ è½½embeddingså’Œmetadata"""
    embeddings_file = config.OUTPUT_DIR / 'schema_embeddings.npz'
    
    if not embeddings_file.exists():
        print(f"âŒ æ‰¾ä¸åˆ°embeddingæ–‡ä»¶: {embeddings_file}")
        return None, None, None
    
    data = np.load(embeddings_file, allow_pickle=True)
    embeddings = data['embeddings']
    metadata = data['metadata']
    
    # åŠ è½½åŸå§‹schemasï¼ˆç”¨äºç‰¹å¾æå–ï¼‰
    schemas_file = config.SCHEMAS_FILE
    schemas = None
    if isinstance(schemas_file, str):
        schemas_file = Path(schemas_file)
    if schemas_file.exists():
        with open(schemas_file, 'r', encoding='utf-8') as f:
            schemas = json.load(f)
    else:
        print(f"âš ï¸  Schemaæ–‡ä»¶ä¸å­˜åœ¨: {schemas_file}")
    
    return embeddings, metadata, schemas


def main():
    """ä¸»ç¨‹åº"""
    print("\n" + "=" * 80)
    print("Schema èšç±»ä¸æ ‡ç­¾æå–ç³»ç»Ÿ".center(80))
    print("=" * 80 + "\n")
    
    # åŠ è½½æ•°æ®
    embeddings, metadata, schemas = load_data()
    if embeddings is None:
        return
    
    # åˆ›å»ºèšç±»å™¨
    clusterer = SchemaClusterer(embeddings, metadata, schemas)
    
    # æ‰¾æœ€ä¼˜Kå€¼
    optimal_k = clusterer.find_optimal_k(k_range=range(10, 51, 5))
    
    # æ‰§è¡Œèšç±»
    result = clusterer.cluster_kmeans(n_clusters=optimal_k)
    
    # 2Då¯è§†åŒ–
    clusterer.visualize_clusters_2d(result.labels, result.cluster_labels, method='tsne')
    clusterer.visualize_clusters_2d(result.labels, result.cluster_labels, method='pca')
    
    # ä¿å­˜ç»“æœ
    clusterer.save_results(result)
    
    print("\nâœ… èšç±»å’Œæ ‡ç­¾æå–å®Œæˆï¼")


if __name__ == '__main__':
    main()
