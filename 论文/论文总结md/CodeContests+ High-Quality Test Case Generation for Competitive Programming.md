# ***CodeContests+ ：High-Quality Test Case Generation for Competitive Programming***

## 一、主要内容

**研究背景**

- 竞赛编程（Competitive Programming）常被用来评测大语言模型（LLM）的推理和代码能力。
- 公共题目和代码解法数据很多，但测试用例（test cases）往往无法公开获取。
- 现有方法生成的测试用例质量不足，存在覆盖率差、错误用例多等问题，导致评测不准。

**提出的方法：生成-验证（Generator-Validator, G-V）Agent 系统**

- **生成器（Generator Agent）**：基于题目生成多样化的输入数据，包括随机数据、边界情况和高强度大数据。
- **验证器（Validator Agent）**：检查生成的用例是否符合题目的输入约束，反馈问题并指导生成器改进。
- 该系统能保证测试用例既覆盖全面，又符合题目要求。

**构建的新数据集 CodeContests+**

- 在原有 **CodeContests** 数据集的基础上，替换了低质量的变异生成用例，使用 G-V 系统生成高质量用例。
- 并提供了**多版本（1x~5x）**预生成用例，数量可控。
- 引入 **Checker Agent**，解决了多解题目（如拓扑排序可有多种答案）无法用简单字符串对比判分的问题。

**实验结果与验证**

- 使用 **172 万条真实选手提交记录**，通过真阳性率（TPR）和真阴性率（TNR）验证测试用例质量。
- CodeContests+ 的评测准确度远超原始 CodeContests，特别是在减少“误判正确解答为错误”的情况上表现突出。
- 在 RL 训练中，基于 CodeContests+ 的模型效果明显优于基于旧数据集的模型。

**贡献与意义**

- 提出首个面向竞赛编程的 LLM **生成-验证 Agent 系统**。
- 发布了 **CodeContests+**，首个拥有经过验证高质量测试用例的大规模竞赛编程数据集。
- 实验证明：高质量测试用例对 RL 训练和模型性能提升有重大作用。
- 该方法未来可扩展到超过 **10 万道公开题目**，为 LLM 提供更强的推理和编程训练基础。

## 二、阅读记录

1.构建测试用例的方法：人工、基于mutaion、基于大语言模型。

2.当前一些自动化方法生成的测试用例主要有覆盖率不足和错误用例多两大问题。覆盖率不足是说一些用例不能涉及题目中的边界情况，不能测试选手答案能否满足时间复杂度和空间复杂度。错误用例多是说一些用例不满足题目要求的约束条件。

3.工作流程：向大模型输入题目描述，让它找出题目中的约束条件，并生成对应的生成测试用例的程序以及相应的若干个运行指令，然后由验证器通过上述运行指令运行程序，并将报错信息（如果有）返回给大模型，让它继续改进生成程序以及运行指令，直到生成的测试用例都没有错误。