### **基于大语言模型的试题自动生成路径研究：核心洞察与方法总结**

本研究旨在探索并验证一种利用大语言模型（LLM）自动生成试题的有效路径，以应对传统人工命题效率低、更新慢等挑战，并提升教育考试的数字化水平。论文提出了一种名为 **GQR（Generation based on Query and Retrieval）**的结构化方法，并通过实验验证了其在中文教育场景下的有效性。

#### **1. 研究背景与挑战**

- **智慧教育与考试的需求：** 随着智慧教育的快速发展，对海量、高质量、个性化的试题需求日益增长。传统人工命题耗时耗力，且经验不足的教师难以胜任。
- **LLM的潜力与局限：** 大语言模型在自然语言理解与生成方面展现出强大能力，为试题自动生成提供了前景。然而，LLM普遍缺乏专业知识，难以达到专业课程的教学精度和难度要求，且存在“幻觉”现象，影响其在教育领域的应用。
- **现有研究的不足：** 早期AQG（Automatic Question Generation）方法受限于模型复杂度；GPT等模型虽有进展，但仍需优化以适应教育实践。

#### **2. 核心方法：GQR 自动生成路径**

本研究提出的 GQR 路径主要包含以下三个关键步骤，旨在克服LLM在专业知识和生成质量上的局限：

**2.1. 针对大模型开展知识增强（Retrieval-Augmented Generation, RAG）**

- **问题：** LLM在缺乏事实知识时可能产生“幻觉”，且无法获取最新的外部信息。

- 解决方案：

   

  采用检索增强生成（RAG）技术，在不更改模型参数的情况下，集成外部知识以提高LLM的准确性和可解释性。

  - 步骤：
    1. **知识库构建：** 将学科知识（如《信息检索》和《数据结构》课程的教材、PPT、在线视频等）转化为数值表示的向量形式，并存储在向量数据库中。
    2. **检索机制：** 利用最大内积搜索（MIPS）技术，根据用户提问（Query）检索出与问题最相关的文档片段（top-k文档）。
    3. **提示增强：** 将检索到的文档片段作为提示的一部分，填入预先构建好的提示模板，作为LLM生成试题的输入，以确保LLM在命题前掌握所有必要信息。

- **效果：** 实验对比显示，知识增强后，LLM对同一问题的回答与标准答案更加接近，显著改善了回答不符合学科背景、内容不完整或错误的问题。

**2.2. 形成和使用结构化的知识点**

- **问题：** 传统教材的单元化模式不利于LLM理解知识间的逻辑和层次，难以形成明确、简洁的提示。

- 解决方案：

   

  将课程内容按知识点形式进行结构化组织，并与LLM应用接口（API）结合。

  - 步骤：
    1. **专家指导：** 在教师和专业人士的指导下，将课程内容细分为具体的知识点，并以双向细目表的形式呈现，包含知识点名称和考查目标。
    2. **自动化填充：** 通过调用LLM API，遍历存储知识点的文件，将单个知识点作为提示模板的一部分进行自动填充。
    3. **保密性：** 确保API调用通过用户密钥控制，并对数据传输和存储进行高等级加密。

- **效果：** 结构化的知识点使LLM能更好地理解命题意图，生成更符合教育目标的试题。

**2.3. 设计试题自动生成的提示模板（Prompt Engineering）**

- **问题：** LLM的输出质量受提示（Prompt）设计影响，需要优化提示来提高准确性和相关性。

- 解决方案：

   

  结合已有研究和多轮试验，设计出符合中文教育场景的提示模板。

  - 关键提示方法：
    - **指令（Giving Instructions）：** 提供清晰的指令，明确LLM的任务。
    - **角色提示（Role-Prompting）：** 为LLM设定明确的角色（如“你是一位教授[科目]这门课的老师”），以引导其输出符合特定语境。
  - **最终指令模板：** “你是一位教授[科目]这门课的老师，现在想要考查学生对所学的[top-k文档在代码中的参数]中知识的掌握程度，根据该目的，你要执行下列任务：1)分析关于[知识点]的內容；2)编制一道[题型]；3)给出正确答案和解释。”

- **效果：** 清晰的角色和指令提示在中文教育场景下表现出良好效果，能更精准高效地生成问题。

#### **3. 试题质量评估与研究局限**

- **评估方法：** 采用五项合格标准（准确性、模板黏性、完善性、试题质量、道德性）计算合格率；通过难度指标P值（0.67，中等难度）评估整体难度；进行学生作答体验问卷调查（真实度高、难度适中）。
- 评估结果：
  - **合格率：** 自动生成的547道试题中，有473题符合全部标准，**总合格率为86.47%**。
  - **难度：** 随机抽取试题组成的测试卷难度P值为0.67，处于中等水平。
  - **用户接受度：** 学生普遍认为试题符合课程考核要求，内容清晰、实用性强，接近真实考题。
- 研究局限：
  1. LLM生成单个试题的**难度难以控制**。
  2. LLM遵循固定模板的能力有待提升，生成具有五个备选项的选择题时，满意度较低。
  3. API调用方式在生成参考答案的**代码缩进和图画方面能力欠缺**，目前主要局限于文字类题型。

#### **4. 结论与未来展望**

本研究成功构建并验证了一种基于大语言模型的试题自动生成路径 GQR。该路径通过 **RAG 进行知识增强**、**结构化知识点整合**以及**精细化提示工程**，显著提升了LLM在教育场景下生成试题的质量和准确性。

**未来研究方向：**

- 探索重新构建或微调专用模型，以更适应自动命题场景。
- 研究更多灵活的任务提示模板，持续改进LLM输出。
- 尝试多轮对话机制以改善LLM的命题质量。