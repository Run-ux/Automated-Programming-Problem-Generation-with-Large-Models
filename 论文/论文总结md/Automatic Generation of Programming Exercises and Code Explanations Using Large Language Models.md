# Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models

------

### **论文核心总结：利用大型语言模型自动生成编程练习和代码解释**

这篇论文《Automatic Generation of Programming Exercises and Code Explanations Using Large Language Models》探讨了利用大型语言模型（LLMs）在**编程教育领域**中自动生成**编程练习**和**代码解释**的潜力。作者以 **OpenAI Codex** 为主要工具，定性和定量地评估了其生成内容的**合理性 (sensibleness)**、**新颖性 (novelty)** 和**实用性 (readiness for use)**。

------

#### **1. 核心研究问题 (Research Questions)**

论文围绕两个主要研究问题展开：

- **RQ1：** 使用 OpenAI Codex 创建的编程练习在多大程度上是合理、新颖和易于应用的？
- **RQ2：** OpenAI Codex 生成的针对入门编程练习的代码解决方案的自然语言解释，其全面性和准确性如何？

------

#### **2. 方法论 (Methodology)**

作者通过以下步骤进行研究：

- **选择大型语言模型：** 使用 OpenAI Codex (基于 GPT-3 且针对代码进行微调的模型)，特别是 `code-davinci-001` 版本，因为它在实验时是最强大的模型。
- 输入选择 (Priming)：
  - **生成编程练习：** 采用包含**问题描述、示例解决方案和自动化测试**的完整“引子”(priming)。此外，通过提供**编程相关概念**（如 `conditional`, `loop`, `class`）和**上下文主题**（如 `hiking`, `fishing`）来引导 Codex 生成更具体、有主题的练习。
  - **生成代码解释：** 侧重于生成**分步式 (step-by-step) 代码解释**，这种解释与 SOLO 分类法中的多结构层次 (multistructural level) 相符，常见于学生被要求解释代码时。引子通常包含源代码，后跟“Step-by-step explanation of the above program:”和编号列表。
- 评估方法：
  - 编程练习评估：
    - **定性分析：** 随机选择 120 个练习，由四位研究人员共同评估其**合理性**（问题是否描述了学生可解决的实际问题）、**新颖性**（在线搜索是否找不到相同或相似的练习）和**实用性**（教师需要进行多少手动修改）。还评估了引子中提供的概念（如 `function`/`class`、`list`/`dictionary`、`context`）在生成练习中的体现情况。
    - **定量分析：** 对所有 240 个练习进行程序化评估，包括：示例解决方案是否可运行、是否通过了自动化测试，以及自动化测试的代码覆盖率。
  - **代码解释评估：** 对 20 个生成的代码解释进行**联合分析**，评估是否所有代码部分都得到了解释，以及每行解释是否正确。评估标准相当严格，要求语言精确。

------

#### **3. 主要结果 (Key Results)**

- 编程练习生成 (RQ1)：

  - **质量：** 75.0% 的练习被认为是**合理**的，81.8% 是**新颖**的，76.7% 具有匹配的示例解决方案。

  - **上下文和概念整合：** Codex 能够很好地将引子中提供的**编程概念**和**上下文主题**融入到新生成的问题描述中，这让生成的练习具有高度定制化和相关性。

  - 实用性挑战：

     

    尽管内容质量高，但练习

    很少能“即插即用”

    ，通常需要人工调整。主要问题包括：

    - 问题描述未能充分讨论**边界条件**。
    - 许多练习**缺乏测试或测试存在缺陷**。
    - 常见错误包括打印 (print) 和返回 (return) 值之间的混淆，以及测试期望的特定数字与输入不符。

  - **可修复性：** 作者指出，这些缺陷通常是微小的，可以通过教师的少量调整来修复，或通过多次生成尝试来获得更好的测试。

- 代码解释生成 (RQ2)：

  - **全面性：** 90% 的代码解释覆盖了所有代码部分。
  - **准确性：** 在 174 行解释中，有 117 行 (67.2%) 是**正确**的。
  - **常见错误：** 不准确的解释主要与**比较和分支条件**的错误解释有关（例如，Codex 将“speed > 100”解释为“if speed is less than 100”）。另一个问题是 `while` 循环结束后程序仍在执行其余部分，但解释却提到“程序在用户输入后结束”。
  - **特定代码表现：** 包含货币转换器类和其用法的代码，其生成的解释中没有错误。
  - **价值：** 尽管存在不准确性，但生成的解释仍可作为师生讨论的良好起点，辅助学生调试。

------

#### **4. 讨论与未来工作 (Discussion & Future Work)**

- **教学工具潜力：** 论文认为 LLMs（如 Codex）在自动生成编程练习和代码解释方面具有巨大潜力，可作为教师的强大工具，减轻其创建教学材料的负担。它们能够生成**新颖且与特定概念和主题相关**的练习，有助于解决现有题库的局限性。
- **监督与微调：** 虽然生成内容质量高，但仍需**人工监督**以确保最终交付给学生的材料的准确性和完整性，尤其是在处理边界条件和测试用例方面。
- **学习活动设计：** 未完全准备好的练习可作为学生学习活动的基础，例如引导学生讨论边界条件或完善问题规范。
- **个性化学习：** Codex 自动根据上下文定制问题描述的能力，为**个性化学习**（根据学生兴趣生成练习）提供了可能性，未来可探讨学生对此类定制问题的看法。
- **代码解释的应用：** 尽管代码解释存在不准确性，但它们仍可作为教学助手与学生进行**坐式对话 (sit-down conversations)** 的起点，帮助学生理解或调试代码。未来工作可探索如何将这些解释转化为**多项选择题**或 **Parsons 问题**，以促进学生反思和评估。
- **“Robosourcing”概念：** 提出“Robosourcing”概念，即利用 AI 生成学习材料，然后由学生评估，这有望解决传统众包中学生创建内容积极性不足的问题。
- **局限性：** 研究样本量相对较小，且主要集中于 Python 语言和入门级练习。未来研究应探索更广泛的语言、更复杂的练习以及不同提示对生成质量的影响。

------

**总结：**

这篇论文有力地论证了 OpenAI Codex 在自动生成编程练习和代码解释方面的巨大潜力，为编程教育带来了新的机遇。尽管仍存在一些质量和准确性问题，需要人工监督和后期调整，但该技术为教师提供了强大的工具，有望提高教学效率，支持个性化学习，并开创“Robosourcing”等新型学习活动。