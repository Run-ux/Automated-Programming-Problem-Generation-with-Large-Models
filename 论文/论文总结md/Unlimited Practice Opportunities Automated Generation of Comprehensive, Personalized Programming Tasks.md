# 《Unlimited Practice Opportunities: Automated Generation of Comprehensive, Personalized Programming Tasks》

---

### **论文核心总结：自动化生成个性化编程任务以提升学习体验**

这篇论文的核心目标是**引入、评估并优化一个名为 "Tutor Kai" 的新功能，该功能能够自动化生成全面、个性化的编程任务，旨在为计算机科学学生提供无限练习机会，同时减轻教师的工作负担**。研究聚焦于评估这些生成任务的质量以及学生对个性化任务的看法。

#### **1. 研究背景与动机 (Introduction & Motivation)**

*   **编程实践的重要性：** 编程是计算机科学教育的基础，学生通过动手实践和经验积累来学习。
*   **传统任务设计的挑战：** 设计高质量的编程任务（尤其是结合学生个人兴趣和文化背景的任务）对教师来说是一项艰巨的任务，因为学生群体多样，教师资源有限。
*   **生成式AI (GenAI) 的潜力：** GenAI在计算教育领域展现出巨大潜力，包括提供自动化、个性化的反馈以及自动生成教育材料（如编程任务）。
*   **现有研究的不足：** 尽管已有研究探索了生成式AI在编程任务方面的应用（如上下文关联任务），但在任务质量和学生视角方面仍有待深入探索。许多现有系统在代码质量（如模型解决方案与单元测试不匹配）方面存在问题。

#### **2. Tutor Kai 任务生成管道 (Task Generation Pipeline in Tutor Kai)**

论文介绍了一个创新的生成管道（见图1），它能够根据学生选择的编程概念（如递归）和上下文（如音乐）创建完整的编程任务。这些任务包含自动化评估和反馈所需的所有组件：

*   **任务描述 (Task Description)：** 文本形式的问题描述，具有上下文关联性，作为学生的主要指令。
*   **代码骨架 (Code Skeleton)：** 提供方法签名和注释的基本结构，指示解决方案的实现位置。
*   **单元测试 (Unit Tests)：** 自动化测试用例，通过比较预期输出和实际输出来验证学生解决方案。
*   **模型解决方案 (Model Solution)：** 一个完全实现的、可执行的解决方案，用于为教师提供参考，并在生成过程中作为内部验证机制，以确保单元测试的正确性。

**生成技术：** 该系统使用了多种提示技术，包括角色提示 (role prompting)、风格提示 (style prompting)、少量示例 (few-shot examples) 和反射 (reflection)。反射组件会自动执行模型解决方案与单元测试，并根据结果反馈给GenAI系统进行改进，最多迭代五次。

#### **3. 评估方法 (Methodology)**

研究采用两阶段混合方法：

*   **第一阶段：专家和自动化评估 (Phase 1: Expert and Automated Assessment)**
    *   **任务生成：** 自动生成200个Python编程任务（100个单概念，50个双概念，50个三概念）。
    *   **评估标准 (E1-E6)：** 专家根据以下六个标准对任务质量进行二元分类（符合/不符合）：
        *   **E1 功能性任务 (Functional Task)：** 模型解决方案和单元测试语法正确且可执行，模型方案通过所有单元测试。
        *   **E2 可解决性任务 (Solvable Task)：** 任务描述提供了完整、明确的实现规范。
        *   **E3 编程概念 (Programming Concepts)：** 任务整合了所有预期的编程概念。
        *   **E4 上下文关联任务 (Contextualized Task)：** 任务描述与请求的上下文匹配。
        *   **E5 模型解决方案 (Model Solution)：** 模型解决方案正确实现了任务规范。
        *   **E6 单元测试 (Unit tests)：** 单元测试准确反映并测试了任务描述中的显式要求。
*   **第二阶段：学生评估 (Phase 2: Student Evaluation)**
    *   **参与者：** 26名计算机科学学生参与。
    *   **任务：** 学生自由选择编程概念和上下文来生成个性化任务，并尝试解决它们。
    *   **数据收集：** 学生对任务的三个方面（A1-A3，关于上下文关联性、问题描述合理性和信息完整性）进行5点Likert量表评分。最后通过一份问卷（B1-B4）评估整体体验（如定制化、有用性）。

#### **4. 主要研究发现 (Results)**

*   **专家评估 (RQ1)：**
    *   **功能性 (E1)：** 89.5%的任务在最多五次迭代后是功能性的，远高于现有研究（如Sarsa et al. [29]的30%）。
    *   **可解决性 (E2)：** 92.5%的任务被评为可解决的。主要问题是方法要求规范不足。
    *   **编程概念 (E3)：** 成功整合所有请求概念的比例随概念数量增加而下降：单概念任务为94%，双概念任务为68%，三概念任务为40%。
    *   **上下文关联性 (E4)：** 所有200个任务描述都成功整合了所选上下文（100%）。
    *   **单元测试 (E6)：** 80.0%的可解决任务具有正确实现的单元测试。常见问题包括测试用例中不正确的比较值或测试超出任务描述要求的情况。
    *   **模型解决方案 (E5)：** 85.4%的模型解决方案正确解决了任务。问题包括“范围蔓延” (scope creep) 和格式错误。
*   **学生评估 (RQ2)：**
    *   **概念与上下文选择：** 学生输入的概念和上下文多样，最常见的是for循环、递归和while循环。运动相关主题是最常选择的上下文。
    *   **任务评价 (A1-A3)：** 学生对上下文关联性 (A1) 和问题描述合理性 (A2) 给予高分（平均分约4.4），89.4%的任务被评为可解决。
    *   **整体体验 (B1-B4)：** 学生对个性化功能（如定制化上下文、定制化编程概念、无限任务生成）的满意度非常高，并认为这些功能对学习过程有益（平均分均在4.25以上，标准差较低）。

#### **5. 讨论与结论 (Discussion & Conclusion)**

*   **系统优势：** Tutor Kai在生成功能性任务方面表现出色，优于现有系统。上下文整合度高，且学生对个性化功能评价积极。
*   **挑战：** 尽管有这些优点，但也存在挑战，如多概念任务的整合率下降，以及单元测试和模型解决方案中可能出现的“范围蔓延”问题，即测试或解决方案超出了任务描述的明确要求。
*   **风险：** 自由文本输入可能带来“提示注入”风险，学生可能试图操纵生成过程以获得更简单的任务。
*   **建议：** 论文建议采用“教师在环” (educator-in-the-loop) 方法，即教师在部署前审查生成的任务，以确保任务的可靠性和安全性。
*   **未来工作：** 包括进一步提高任务质量、调整任务难度以适应学生能力和兴趣、以及研究任务个性化与学生动机之间的关系。

**总结来说，这篇论文提出并验证了一个有效利用GenAI技术来自动化生成个性化、全面的编程任务的系统。尽管在多概念任务整合和防止“范围蔓延”方面仍有改进空间，但该系统在任务质量和学生满意度方面都取得了显著成果，为计算机科学教育带来了无限的实践机会和潜在的学习效益。**